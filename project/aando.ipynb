{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import nltk\n",
    "import re\n",
    "#import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='три', tag=OpencorporaTag('NUMR inan,accs'), normal_form='три', score=0.571428, methods_stack=((<DictionaryAnalyzer>, 'три', 2907, 4),)), Parse(word='три', tag=OpencorporaTag('NUMR nomn'), normal_form='три', score=0.285714, methods_stack=((<DictionaryAnalyzer>, 'три', 2907, 0),)), Parse(word='три', tag=OpencorporaTag('VERB,impf,tran sing,impr,excl'), normal_form='тереть', score=0.142857, methods_stack=((<DictionaryAnalyzer>, 'три', 2339, 11),))]\n",
      "[Parse(word='великой', tag=OpencorporaTag('ADJF femn,sing,gent'), normal_form='великий', score=0.655172, methods_stack=((<DictionaryAnalyzer>, 'великой', 668, 8),)), Parse(word='великой', tag=OpencorporaTag('ADJF femn,sing,ablt'), normal_form='великий', score=0.137931, methods_stack=((<DictionaryAnalyzer>, 'великой', 668, 11),)), Parse(word='великой', tag=OpencorporaTag('ADJF femn,sing,datv'), normal_form='великий', score=0.103448, methods_stack=((<DictionaryAnalyzer>, 'великой', 668, 9),)), Parse(word='великой', tag=OpencorporaTag('ADJF femn,sing,loct'), normal_form='великий', score=0.103448, methods_stack=((<DictionaryAnalyzer>, 'великой', 668, 13),))]\n",
      "[Parse(word='зачем', tag=OpencorporaTag('ADVB,Ques,Prdx'), normal_form='зачем', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'зачем', 1578, 0),))]\n",
      "[Parse(word='маму', tag=OpencorporaTag('NOUN,anim,femn sing,accs'), normal_form='мама', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'маму', 1907, 3),))]\n"
     ]
    }
   ],
   "source": [
    "huis = [\"три\", \"великой\", \"зачем\", \"маму\"]\n",
    "for hui in huis:\n",
    "    print(morph.parse(hui))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "#print(tokens)\n",
    "\n",
    "def goldenit(m):\n",
    "    tags_cl = str(m.tag)\n",
    "    if ' ' in tags_cl or ',' in tags_cl:\n",
    "        ts = re.sub(' ', ',', tags_cl)\n",
    "        tags = ts.split(',')\n",
    "        mPOS = tags[0]\n",
    "        nPOS = ','.join(tags[1:])\n",
    "    else:\n",
    "        nPOS = ''\n",
    "        mPOS = str(m.tag)\n",
    "    gold = (m.word, m.normal_form, mPOS, nPOS)\n",
    "    return gold\n",
    "\n",
    "golden_st = []\n",
    "\n",
    "for token in tokens:\n",
    "    m = morph.parse(token)[0]\n",
    "    if m.score >= 0.95:\n",
    "        gold = goldenit(m)\n",
    "        golden_st.append(gold)\n",
    "    else:\n",
    "        for item in morph.parse(token):\n",
    "            gold = goldenit(item)\n",
    "            golden_st.append(gold)\n",
    "    \n",
    "\n",
    "def wrtitdown(fname, ready_tokens):\n",
    "    with open(fname, 'w', encoding='utf-8') as f:\n",
    "        f.write('Wordform_GS2\\tLemma_GS2\\tPOS_GS2\\tGram_GS2\\n')\n",
    "        for token in ready_tokens:\n",
    "            mes = '\\t'.join(token)\n",
    "            mes += '\\n'\n",
    "            f.write(mes)\n",
    "            \n",
    "wrtitdown('pymorph_standard.txt', golden_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................................] 40616122 / 40616122"
     ]
    }
   ],
   "source": [
    "model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "modelfile = wget.download(model_url)\n",
    "model = Model.load(modelfile)\n",
    "process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(data):\n",
    "    processed = process_pipeline.process(data)\n",
    "    content = [i for i in processed.split('\\n') if not i.startswith('#')]\n",
    "    return content\n",
    "with open('output.conllu', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(parsing(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impf,tran,pres\n"
     ]
    }
   ],
   "source": [
    "cncn = morph.parse('автобрея')[0].tag\n",
    "cnc = re.sub(' ', ',', str(cncn))\n",
    "tatat = cnc.split(',')\n",
    "itogog = ','.join(tatat[1:])\n",
    "print(itogog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='парными', tag=OpencorporaTag('ADJF plur,ablt'), normal_form='парный', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'парными', 34, 25),)),\n",
       " Parse(word='парными', tag=OpencorporaTag('NOUN,inan,femn plur,ablt'), normal_form='парная', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'парными', 115, 11),)),\n",
       " Parse(word='парными', tag=OpencorporaTag('ADJF plur,ablt'), normal_form='парной', score=0.3333333333333333, methods_stack=((<DictionaryAnalyzer>, 'парными', 136, 25),))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('парными')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v\\tb\\t\\t'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n3 = ('v', 'b', '', '')\n",
    "'\\t'.join(n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# newdoc', '# newpar', '# sent_id = 1', '# text = Цветки дикорастущих форм обыкновенной сирени мелкие, собраны парными пучками (по 3 - 5 цветков в пучке, отдельные сорта по 7 - 9 цветков).', '1\\tЦветки\\tЦветка\\tNOUN\\t_\\tAnimacy=Inan|Case=Nom|Gender=Fem|Number=Plur\\t8\\tnsubj:pass\\t_\\t_', '2\\tдикорастущих\\tдикорастущий\\tADJ\\t_\\tCase=Gen|Degree=Pos|Number=Plur\\t3\\tamod\\t_\\t_', '3\\tформ\\tформа\\tNOUN\\t_\\tAnimacy=Inan|Case=Gen|Gender=Fem|Number=Plur\\t1\\tnmod\\t_\\t_', '4\\tобыкновенной\\tобыкновенный\\tADJ\\t_\\tCase=Gen|Degree=Pos|Gender=Fem|Number=Sing\\t5\\tamod\\t_\\t_', '5\\tсирени\\tсирена\\tNOUN\\t_\\tAnimacy=Inan|Case=Gen|Gender=Fem|Number=Sing\\t3\\tnmod\\t_\\t_', '6\\tмелкие\\tмелкий\\tADJ\\t_\\tCase=Nom|Degree=Pos|Number=Plur\\t1\\tamod\\t_\\tSpaceAfter=No']\n"
     ]
    }
   ],
   "source": [
    "gold_pipe = []\n",
    "\n",
    "with open('processed (1).conllu', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    print(lines[:10])\n",
    "    for line in lines:\n",
    "        if re.match(r'\\d\\d*\\t', line):\n",
    "            parts = line.split('\\t')\n",
    "            if parts[5] == '_':\n",
    "                nPOS = ''\n",
    "            else:\n",
    "                nPOS_list = []\n",
    "                for pair in parts[5].split('|'):\n",
    "                    nPOS_list.append(pair.split('=')[1].lower())\n",
    "                nPOS = ','.join(nPOS_list)\n",
    "            g = (parts[1], parts[2], parts[3], nPOS)\n",
    "            gold_pipe.append(g)\n",
    "            \n",
    "wrtitdown('golden_pipe.txt', gold_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='путча', tag=OpencorporaTag('NOUN,inan,masc sing,gent'), normal_form='путч', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'путча', 79, 1),))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('путча')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "a[1:-1]\n",
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "a = '12345'\n",
    "\n",
    "for c in range(0, len(a)):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:NOUN\t:\t356\n",
      "pr:PREP\t:\t128\n",
      "a:ADJF\t:\t127\n",
      "v:VERB\t:\t86\n",
      "conj:CONJ\t:\t58\n",
      "adv:ADVB\t:\t40\n",
      "spro:NPRO\t:\t32\n",
      "apro:ADJF\t:\t31\n",
      "v:INFN\t:\t29\n",
      "adv:PRCL\t:\t21\n",
      "v:PRTF\t:\t12\n",
      "v:ADJF\t:\t11\n",
      "apro:NPRO\t:\t10\n",
      "v:PRTS\t:\t7\n",
      "s:ADJS\t:\t5\n",
      "adv:PRED\t:\t5\n",
      "a:ADJS\t:\t4\n",
      "adv:CONJ\t:\t4\n",
      "s:UNKN\t:\t4\n",
      "v:GRND\t:\t4\n",
      "a:NOUN\t:\t3\n",
      "anum:ADJF\t:\t3\n",
      "adv:ADJS\t:\t2\n",
      "v:NOUN\t:\t2\n",
      "num:NUMR\t:\t2\n",
      "spro:CONJ\t:\t2\n",
      "adv:COMP\t:\t2\n",
      "adv:NOUN\t:\t1\n",
      "s:VERB\t:\t1\n",
      "s:ADVB\t:\t1\n",
      "anum:NOUN\t:\t1\n",
      "spro:ADJF\t:\t1\n",
      "spro:PRCL\t:\t1\n",
      "s:ADJF\t:\t1\n",
      "apro:ADJS\t:\t1\n",
      "spro:ADVB\t:\t1\n",
      "a:PRTF\t:\t1\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "fnames = ['GoldStandard.txt', 'pymorph_standard_nodouble.txt']\n",
    "\n",
    "def getlines(fn):\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        text = re.sub('ё', \"е\", text)\n",
    "        text = re.sub(\"Ё\", \"е\", text)\n",
    "        lines = text.split('\\n')[1:-1]\n",
    "    return lines\n",
    "\n",
    "GSr = getlines(fnames[0])\n",
    "PMr = getlines(fnames[1])\n",
    "\n",
    "GS = []\n",
    "\n",
    "for line in GSr:\n",
    "    if not re.search('\\t\\t\\t', line):\n",
    "        GS.append(line.lower())\n",
    "\n",
    "i = 0\n",
    "PM = []\n",
    "\n",
    "for line in PMr:\n",
    "    word = GS[i].split('\\t')[0]\n",
    "    word = re.sub('\\.', '', word)\n",
    "    w2 = line.split('\\t')[0]\n",
    "    if word == w2:\n",
    "        PM.append(line)\n",
    "        i += 1\n",
    "        \n",
    "PM = PM[:1000]\n",
    "GS = GS[:1000]\n",
    "        \n",
    "with open('1000pm.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in PM:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "        \n",
    "with open('1000gs.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in GS:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "        \n",
    "\n",
    "\n",
    "cntdict = {}\n",
    "        \n",
    "for c in range(0, 1000):\n",
    "    pmp = PM[c].split('\\t')[2]\n",
    "    gsp = GS[c].split('\\t')[2]\n",
    "    key = '%s:%s' % (gsp, pmp)\n",
    "    if key not in cntdict:\n",
    "        cntdict[key] = 1\n",
    "    else:\n",
    "        cntdict[key] += 1\n",
    "        \n",
    "total = 0\n",
    "        \n",
    "for key in sorted(cntdict, key=cntdict.get, reverse=True):\n",
    "    mes = '%s\\t:\\t%d' % (key, cntdict[key])\n",
    "    print(mes)\n",
    "    total += cntdict[key]\n",
    "    \n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "932"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr = []\n",
    "rg = 0\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    if GS[i].split('\\t')[1].lower() == PM[i].split('\\t')[1].lower():\n",
    "        rg += 1\n",
    "    else:\n",
    "        wr.append((GS[i].split('\\t')[1].lower(), PM[i].split('\\t')[1].lower()))\n",
    "        \n",
    "rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "s = (11, 10, 4, 4, 3, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1)\n",
    "for c in s:\n",
    "    i += c\n",
    "\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['в', ['']]\n",
      "35\n",
      "[7, ['собраны', 0.75]]\n",
      "[39, ['морозостоек', 0.667]]\n",
      "[108, ['полтриллиона', 0.667]]\n",
      "[181, ['сел', 0.0]]\n",
      "[189, ['готовы', 0.5]]\n",
      "[204, ['году', 0.667]]\n",
      "[221, ['все', 0.0]]\n",
      "[272, ['виду', 0.667]]\n",
      "[283, ['запущен', 0.8]]\n",
      "[298, ['сделаны', 0.75]]\n",
      "[331, ['редки', 0.5]]\n",
      "[378, ['этого', 0.667]]\n",
      "[417, ['чем-либо', 0.5]]\n",
      "[445, ['состояния', 0.333]]\n",
      "[460, ['все', 0.5]]\n",
      "[499, ['нужна', 0.667]]\n",
      "[515, ['газеты', 0.333]]\n",
      "[518, ['агентства', 0.333]]\n",
      "[561, ['борзых', 0.0]]\n",
      "[567, ['гляди', 0.5]]\n",
      "[588, ['этого', 0.667]]\n",
      "[624, ['сотвори', 0.5]]\n",
      "[629, ['люби', 0.5]]\n",
      "[640, ['скорби', 0.5]]\n",
      "[670, ['тартасов', 0.333]]\n",
      "[678, ['таков', 0.667]]\n",
      "[702, ['собой', 0.0]]\n",
      "[729, ['стороны', 0.333]]\n",
      "[804, ['принято', 0.8]]\n",
      "[825, ['сказанным', 0.8]]\n",
      "[830, ['совершенного', 0.6]]\n",
      "[840, ['все', 0.5]]\n",
      "[879, ['записи', 0.667]]\n",
      "[880, ['решения', 0.333]]\n",
      "[893, ['действия', 0.333]]\n"
     ]
    }
   ],
   "source": [
    "tags = {}\n",
    "\n",
    "with open('tagg.txt', 'r', encoding='utf-8') as f:\n",
    "    plain = f.read()\n",
    "    lines = plain.split('\\n')\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            gstag, pmtag = line.split(':')\n",
    "            tags[pmtag] = gstag\n",
    "\n",
    "amb = []\n",
    "\n",
    "with open('1000pm_ambig.txt', 'r', encoding='utf-8') as f:\n",
    "    plain = f.read()\n",
    "    lines = plain.split('\\n')\n",
    "    for line in lines:\n",
    "        if not 'NUMB' in line:\n",
    "            if not 'PNCT' in line:\n",
    "                ln = line.split('\\t')\n",
    "                if len(amb) > 0:\n",
    "                    word = amb[-1][0]\n",
    "                    if word == ln[0]:\n",
    "                        ln.append(0)\n",
    "                    else:\n",
    "                        ln.append(1)\n",
    "                    amb.append(ln)\n",
    "                else:\n",
    "                    ln.append(1)\n",
    "                    amb.append(ln)\n",
    "\n",
    "amb = amb[:-1]\n",
    "    \n",
    "d = {}\n",
    "b = 0\n",
    "for line in amb:\n",
    "    b += line[4]\n",
    "    if b not in d:\n",
    "        d[b] = [line[0], [line[3]]]\n",
    "    else:\n",
    "        d[b][1].append(line[3])\n",
    "\n",
    "GSl = []       \n",
    "        \n",
    "for line in GS:        \n",
    "    l = line.split('\\t')\n",
    "    GSl.append(l)\n",
    "    \n",
    "print(len(GSl))\n",
    "    \n",
    "lc = 0\n",
    "\n",
    "nd = {}\n",
    "\n",
    "print(d[12])\n",
    "\n",
    "for key in sorted(d):\n",
    "    word = d[key][0].lower()\n",
    "    word = re.sub('ё', \"е\", word)\n",
    "    worl = GSl[lc][0].lower()\n",
    "    worl = re.sub('ё', \"е\", worl)\n",
    "    worl = re.sub('\\.', '', worl)\n",
    "    if word == worl:\n",
    "        gstags = GSl[lc][3].split(',')\n",
    "        total = 0\n",
    "        for gt in gstags:\n",
    "            if gt in tags.values():\n",
    "                total += 1\n",
    "        acc = []\n",
    "        if d[key][1][0] == '':\n",
    "            fin = 1.0\n",
    "        else:\n",
    "            for var in d[key][1]:\n",
    "                ts = var.split(',')\n",
    "                curacc = 0\n",
    "                for t in ts:\n",
    "                    if t in tags:\n",
    "                        nt = tags[t]\n",
    "                        if nt in gstags:\n",
    "                            curacc += 1\n",
    "                if total > 0:\n",
    "                    acc.append(round(curacc / total, 3))\n",
    "                else:\n",
    "                    acc.append(1.0)\n",
    "            fin = max(acc)\n",
    "        nd[key] = [word, fin]\n",
    "        lc += 1\n",
    "    \n",
    "answer = 0\n",
    "fail = []\n",
    "    \n",
    "for key in nd:\n",
    "    if nd[key][1] < 1.0:\n",
    "        answer += 1\n",
    "        fail.append([key, nd[key]])\n",
    "        \n",
    "print(answer)\n",
    "for fa in fail:\n",
    "    print(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "utags = {}\n",
    "with open('utagg.txt', 'r', encoding='utf-8') as f:\n",
    "    plain = f.read()\n",
    "    lines = plain.split('\\n')\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            gstag, pmtag = line.split(':')\n",
    "            utags[pmtag] = gstag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['юридическим\\tюридический\\tADJ\\tins,pos,neut,sing', 'лицом\\tлицо\\tNOUN\\tinan,ins,neut,sing']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1015"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open('golden_pipe.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    lines = lines[1:1240]\n",
    "    print(lines[-2:])\n",
    "    UDr = []\n",
    "    UD = []\n",
    "    for line in lines:\n",
    "        UDr.append(line.split('\\t'))\n",
    "    for line in UDr:\n",
    "        if line[2] != 'PUNCT':\n",
    "            if line[2] != \"NUM\":\n",
    "                UD.append(line)\n",
    "                \n",
    "with open('1000udp.txt', 'w', encoding='utf-8') as f:\n",
    "    for l in UD:\n",
    "        f.write('\\t'.join(l))\n",
    "        f.write('\\n')\n",
    "\n",
    "len(UD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexeme 937\n",
      "gram 71\n",
      "######\n",
      "NOUN:s\t:\t335\n",
      "VERB:v\t:\t136\n",
      "ADJ:a\t:\t130\n",
      "ADP:pr\t:\t128\n",
      "ADV:adv\t:\t49\n",
      "CCONJ:conj\t:\t46\n",
      "PRON:spro\t:\t36\n",
      "DET:apro\t:\t35\n",
      "PROPN:s\t:\t31\n",
      "PART:adv\t:\t17\n",
      "AUX:v\t:\t10\n",
      "SCONJ:conj\t:\t10\n",
      "PRON:apro\t:\t6\n",
      "ADJ:v\t:\t4\n",
      "ADJ:adv\t:\t4\n",
      "NOUN:a\t:\t3\n",
      "ADJ:anum\t:\t3\n",
      "AUX:adv\t:\t2\n",
      "NUMP:num\t:\t2\n",
      "PART:conj\t:\t2\n",
      "PROPN:a\t:\t1\n",
      "PRON:adv\t:\t1\n",
      "ADV:spro\t:\t1\n",
      "NUMP:anum\t:\t1\n",
      "VERB:s\t:\t1\n",
      "ADJ:s\t:\t1\n",
      "VERB:adv\t:\t1\n",
      "NOUN:v\t:\t1\n",
      "ADJ:apro\t:\t1\n",
      "SCONJ:adv\t:\t1\n",
      "VERB:a\t:\t1\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "nud = {}\n",
    "uc = 0\n",
    "ucntd = {}\n",
    "lexc = 0\n",
    "\n",
    "for item in UD:\n",
    "    word = item[0].lower()\n",
    "    word = re.sub('ё', \"е\", word)\n",
    "    word = re.sub('\\.', '', word)\n",
    "    worl = GSl[uc][0].lower()\n",
    "    worl = re.sub('ё', \"е\", worl)\n",
    "    worl = re.sub('\\.', '', worl)\n",
    "    if '-' in worl:\n",
    "        worl = worl.split('-')[0]\n",
    "    if '-' in word:\n",
    "        word = word.split('-')[0]\n",
    "    if word == worl:\n",
    "        gstags = GSl[uc][3].split(',')\n",
    "        total = 0\n",
    "        for gt in gstags:\n",
    "            if gt in utags.values():\n",
    "                total += 1\n",
    "        acc = []\n",
    "        if item[3] == '':\n",
    "            fin = 1.0\n",
    "        else:\n",
    "            ts = item[3].split(',')\n",
    "            curacc = 0\n",
    "            for t in ts:\n",
    "                if t in utags:\n",
    "                    nt = utags[t]\n",
    "                    if nt in gstags:\n",
    "                        curacc += 1\n",
    "            if total > 0:\n",
    "                acc.append(round(curacc / total, 3))\n",
    "            else:\n",
    "                acc.append(1.0)\n",
    "            fin = max(acc)\n",
    "        nud[uc] = [word, fin]\n",
    "        upos = item[2]\n",
    "        gpos = GSl[uc][2]\n",
    "        key = '%s:%s' % (upos, gpos)\n",
    "        if key not in ucntd:\n",
    "            ucntd[key] = 1\n",
    "        else:\n",
    "            ucntd[key] += 1\n",
    "        ulex = item[1].lower()\n",
    "        glex = GSl[uc][1].lower()\n",
    "        if ulex == glex:\n",
    "            lexc += 1\n",
    "        uc += 1\n",
    "\n",
    "print('lexeme', lexc)\n",
    "        \n",
    "answer = 0\n",
    "fail = []\n",
    "    \n",
    "for key in nud:\n",
    "    if nud[key][1] < 1.0:\n",
    "        answer += 1\n",
    "        fail.append([key, nud[key]])\n",
    "        \n",
    "print('gram', answer)\n",
    "        \n",
    "print('######')\n",
    "        \n",
    "totalu = 0\n",
    "        \n",
    "for key in sorted(ucntd, key=ucntd.get, reverse=True):\n",
    "    mes = '%s\\t:\\t%d' % (key, ucntd[key])\n",
    "    print(mes)\n",
    "    totalu += ucntd[key]\n",
    "    \n",
    "print(totalu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
